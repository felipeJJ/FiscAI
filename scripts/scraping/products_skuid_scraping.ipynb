{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from threading import Lock\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o WebDriver (sem headless)\n",
    "chrome_options = Options()\n",
    "\n",
    "service = Service(\"/opt/homebrew/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Acessar o endpoint\n",
    "driver.get(\"https://www.\"substituir\".com.br/api/catalog_system/pub/category/tree/3\") \n",
    "\n",
    "# Garantir que a página está completamente carregada\n",
    "WebDriverWait(driver, 10).until(\n",
    "    lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    ")\n",
    "\n",
    "# Aguardar até que o elemento <pre> apareça\n",
    "try:\n",
    "    pre_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//pre\"))\n",
    "    )\n",
    "    json_text = pre_element.text  # Recuperar o texto dentro de <pre>\n",
    "\n",
    "    # Converter o texto JSON em uma lista (array)\n",
    "    data_array = json.loads(json_text)\n",
    "\n",
    "    # Exibir o conteúdo do array\n",
    "    print(data_array)\n",
    "\n",
    "    # Salvar o array em um arquivo JSON\n",
    "    with open(\"../../data/raw/dados.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data_array, json_file, ensure_ascii=False, indent=4)  # Salvando o arquivo com indentação\n",
    "\n",
    "    print(\"Dados salvos no arquivo '/data/processed/dados.json'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao localizar o elemento: {e}\")\n",
    "\n",
    "# Fechar o WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo do seu array JSON carregado (substitua pelo seu array real)\n",
    "with open('../../data/raw/dados.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Função recursiva para extrair apenas os links das categorias mais específicas (sem filhos)\n",
    "def extract_leaf_links(data):\n",
    "    leaf_links = []\n",
    "\n",
    "    # Verificar se o objeto atual é um dicionário\n",
    "    if isinstance(data, dict):\n",
    "        # Verificar se tem a chave 'url' e se 'children' está vazio ou não existe\n",
    "        if 'url' in data and ('children' not in data or not data['children']):\n",
    "            leaf_links.append(data['url'])  # Adicionar o link da categoria mais específica\n",
    "            print(f\"Link mais específico encontrado: {data['url']}\")  # Exibir o link encontrado para depuração\n",
    "\n",
    "        # Se tiver filhos em 'children', processar recursivamente\n",
    "        elif 'children' in data and isinstance(data['children'], list):\n",
    "            for child in data['children']:\n",
    "                leaf_links.extend(extract_leaf_links(child))  # Recursão para os filhos\n",
    "    \n",
    "    # Verificar se o objeto atual é uma lista\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            leaf_links.extend(extract_leaf_links(item))  # Processar cada item da lista\n",
    "\n",
    "    return leaf_links\n",
    "\n",
    "# Extrair apenas os links das categorias mais específicas (sem filhos)\n",
    "leaf_links = extract_leaf_links(data)\n",
    "\n",
    "# Verificar se os links foram extraídos corretamente\n",
    "print(f\"Total de links mais específicos extraídos: {len(leaf_links)}\")\n",
    "print(leaf_links)\n",
    "\n",
    "# Salvar os links extraídos em um arquivo\n",
    "with open('../../data/raw/links_extrated.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(leaf_links, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Links salvos em 'data/raw/links_extrated.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista global para acumular todos os skuIds\n",
    "all_sku_ids = []\n",
    "# Caminhos dos arquivos\n",
    "links_extrated = \"../../data/raw/links_extrated.json\"\n",
    "processed_links_array = \"../../data/raw/processed_links.json\"\n",
    "products_ids = \"../../data/raw/products_ids_list.json\"\n",
    "\n",
    "# Mutex para controlar o acesso à lista global em threads\n",
    "lock = Lock()\n",
    "\n",
    "# Função para configurar o WebDriver e processar um link com paginação e retry\n",
    "def process_link(link, max_retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        chrome_options = Options()\n",
    "        service = Service(\"/opt/homebrew/bin/chromedriver\")\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"script\"))\n",
    "            )\n",
    "\n",
    "            # Verificação de mensagem \"Ops, não encontramos nada em sua busca\"\n",
    "            if \"Ops, não encontramos nada em sua busca\" in driver.page_source:\n",
    "                print(f\"Nenhum produto encontrado para o link: {link}\")\n",
    "                # Adicionar o link diretamente ao arquivo de links processados\n",
    "                with lock:\n",
    "                    if not os.path.exists(processed_links):\n",
    "                        with open(processed_links, \"w\") as f:\n",
    "                            json.dump([], f)\n",
    "                    with open(processed_links, \"r\") as f:\n",
    "                        processed_links = json.load(f)\n",
    "                    processed_links.append(link)\n",
    "                    with open(processed_links, \"w\") as f:\n",
    "                        json.dump(processed_links, f, indent=4)\n",
    "                return  # Sair da função, pois não há produtos para processar\n",
    "\n",
    "            # Verificação de paginação e processamento dos produtos\n",
    "            page_numbers = driver.find_elements(By.CSS_SELECTOR, \"ul.pages li.page-number\")\n",
    "            total_pages = int(page_numbers[-1].text) if page_numbers else 1\n",
    "\n",
    "            link_processed_successfully = False  # Indicador de sucesso para o link\n",
    "\n",
    "            for page in range(1, total_pages + 1):\n",
    "                paginated_link = f\"{link}#{page}\"\n",
    "                driver.get(paginated_link)\n",
    "                time.sleep(2)\n",
    "\n",
    "                scripts = driver.find_elements(By.TAG_NAME, \"script\")\n",
    "\n",
    "                for script in scripts:\n",
    "                    script_content = script.get_attribute(\"innerHTML\")\n",
    "\n",
    "                    if '\"shelfProductIds\"' in script_content:\n",
    "                        match = re.search(r'\"shelfProductIds\":\\[(.*?)\\]', script_content)\n",
    "                        if match:\n",
    "                            shelf_product_ids = [sku.strip('\"') for sku in match.group(1).split(',')]\n",
    "                            with lock:\n",
    "                                all_sku_ids.extend(shelf_product_ids)\n",
    "                            print(f\"Link: {paginated_link} - Página {page} - shelfProductIds: {shelf_product_ids}\")\n",
    "                            link_processed_successfully = True\n",
    "                        break\n",
    "\n",
    "            # Registrar o link como processado apenas se algum `skuID` foi coletado\n",
    "            if link_processed_successfully:\n",
    "                with lock:\n",
    "                    if not os.path.exists(processed_links_array):\n",
    "                        with open(processed_links_array, \"w\") as f:\n",
    "                            json.dump([], f)\n",
    "                    with open(processed_links_array, \"r\") as f:\n",
    "                        processed_links_array = json.load(f)\n",
    "                    processed_links_array.append(link)\n",
    "                    with open(processed_links_array, \"w\") as f:\n",
    "                        json.dump(processed_links_array, f, indent=4)\n",
    "\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o link {link} na tentativa {attempt + 1}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(2)\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    print(f\"Falha ao processar o link {link} após {max_retries} tentativas.\")\n",
    "\n",
    "# Função para verificar e tentar reprocessar links faltantes até que todos sejam processados\n",
    "def retry_missing_links():\n",
    "    with open(links_extrated, 'r', encoding='utf-8') as f:\n",
    "        all_links = set(json.load(f))\n",
    "\n",
    "    while True:\n",
    "        with open(processed_links_array, 'r') as f:\n",
    "            processed_links = set(json.load(f))\n",
    "\n",
    "        # Identificar links que não foram processados\n",
    "        remaining_links = list(all_links - processed_links)\n",
    "        if not remaining_links:\n",
    "            break\n",
    "\n",
    "        print(f\"{len(remaining_links)} links não foram processados. Tentando novamente.\")\n",
    "\n",
    "        # Reprocessar os links restantes\n",
    "        with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            executor.map(process_link, remaining_links)\n",
    "\n",
    "# Executar a primeira rodada de scraping\n",
    "with open(links_extrated, 'r', encoding='utf-8') as f:\n",
    "    links = json.load(f)\n",
    "\n",
    "# Executar as requisições em paralelo com ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    executor.map(process_link, links)\n",
    "\n",
    "# Tentar novamente os links que não foram processados com sucesso\n",
    "retry_missing_links()\n",
    "\n",
    "# Salvar todos os skuIds extraídos em um arquivo JSON\n",
    "with open(products_ids, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(all_sku_ids, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Todos os skuIds foram salvos no arquivo 'lista_skuID_produtos.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
