{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão: 1.00\n",
      "Recall: 1.00\n",
      "F-Score: 1.00\n",
      "\n",
      "Resumo dos Acertos e Erros por Tipo de Entidade:\n",
      "PRODUCT_NAME: Correto = 464, Perdido = 0\n",
      "CLUSTER: Correto = 463, Perdido = 1\n",
      "BRAND: Correto = 462, Perdido = 2\n",
      "CATEGORY: Correto = 1507, Perdido = 16\n",
      "PRICE: Correto = 463, Perdido = 0\n",
      "\n",
      "Tipo de Entidade: CATEGORY\n",
      "Precisão Manual: 1.00\n",
      "Recall Manual: 0.99\n",
      "F1-Score Manual: 0.99\n",
      "--------------------------------------------------\n",
      "\n",
      "Tipo de Entidade: CLUSTER\n",
      "Precisão Manual: 1.00\n",
      "Recall Manual: 1.00\n",
      "F1-Score Manual: 1.00\n",
      "--------------------------------------------------\n",
      "\n",
      "Tipo de Entidade: PRODUCT_NAME\n",
      "Precisão Manual: 1.00\n",
      "Recall Manual: 1.00\n",
      "F1-Score Manual: 1.00\n",
      "--------------------------------------------------\n",
      "\n",
      "Tipo de Entidade: PRICE\n",
      "Precisão Manual: 1.00\n",
      "Recall Manual: 1.00\n",
      "F1-Score Manual: 1.00\n",
      "--------------------------------------------------\n",
      "\n",
      "Tipo de Entidade: BRAND\n",
      "Precisão Manual: 1.00\n",
      "Recall Manual: 1.00\n",
      "F1-Score Manual: 1.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model_output_dir = \"../../model/spaCy_model/\"\n",
    "nlp = spacy.load(model_output_dir)\n",
    "\n",
    "# Carregar o conjunto de validação\n",
    "validation_data_path = \"../../data/processed/validation_data.json\"\n",
    "with open(validation_data_path, \"r\") as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "# Preparar as previsões e os rótulos para avaliação\n",
    "true_entities = []\n",
    "pred_entities = []\n",
    "\n",
    "for text, annotations in validation_data:\n",
    "    # Criar o objeto Doc\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Adicionar as entidades reais e previstas\n",
    "    true_ents = [(start, end, label) for start, end, label in annotations[\"entities\"]]\n",
    "    pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Adicionar apenas se o número de entidades coincidir\n",
    "    if len(true_ents) == len(pred_ents):\n",
    "        true_entities.append(true_ents)\n",
    "        pred_entities.append(pred_ents)\n",
    "\n",
    "# Função para converter as entidades em um formato de lista plana para avaliação\n",
    "def flatten_entities(entities_list):\n",
    "    flattened_labels = []\n",
    "    flattened_offsets = []\n",
    "    for entities in entities_list:\n",
    "        for start, end, label in entities:\n",
    "            flattened_labels.append(label)\n",
    "            flattened_offsets.append((start, end))\n",
    "    return flattened_labels, flattened_offsets\n",
    "\n",
    "# Converter as listas de entidades\n",
    "true_labels, true_offsets = flatten_entities(true_entities)\n",
    "pred_labels, pred_offsets = flatten_entities(pred_entities)\n",
    "\n",
    "# Calcular as métricas de precisão, recall e f-score manualmente\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(true_labels, pred_labels, average=\"weighted\", zero_division=\"warn\")\n",
    "print(f\"Precisão: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F-Score: {fscore:.2f}\")\n",
    "\n",
    "# Contar acertos e erros por tipo de entidade\n",
    "correct_counts = defaultdict(int)\n",
    "missed_counts = defaultdict(int)\n",
    "\n",
    "# Comparação direta entre entidades verdadeiras e previstas\n",
    "for true, pred in zip(true_entities, pred_entities):\n",
    "    true_set = set(true)\n",
    "    pred_set = set(pred)\n",
    "    \n",
    "    # Verifica acertos e erros\n",
    "    for entity in true_set:\n",
    "        if entity in pred_set:\n",
    "            correct_counts[entity[2]] += 1  # entity[2] é o nome da entidade\n",
    "        else:\n",
    "            missed_counts[entity[2]] += 1\n",
    "\n",
    "# Resumo dos acertos e erros por tipo de entidade\n",
    "print(\"\\nResumo dos Acertos e Erros por Tipo de Entidade:\")\n",
    "for label in set(true_labels):\n",
    "    print(f\"{label}: Correto = {correct_counts[label]}, Perdido = {missed_counts[label]}\")\n",
    "\n",
    "# # Visualização de Amostras\n",
    "# num_samples = 5\n",
    "# samples = random.sample(validation_data, num_samples)\n",
    "\n",
    "# print(\"\\nAmostras de Avaliação do Modelo no Conjunto de Validação:\\n\")\n",
    "# for i, (text, annotations) in enumerate(samples, 1):\n",
    "#     doc = nlp(text)\n",
    "#     true_entities_sample = [(start, end, label) for start, end, label in annotations[\"entities\"]]\n",
    "#     pred_entities_sample = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "#     print(f\"Amostra {i}:\")\n",
    "#     print(f\"Texto: {text}\\n\")\n",
    "#     print(f\"Entidades Verdadeiras: {true_entities_sample}\")\n",
    "#     print(f\"Entidades Previstas: {pred_entities_sample}\\n\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# Calcular métricas manualmente para cada tipo de entidade\n",
    "for label in correct_counts:\n",
    "    total_true = correct_counts[label] + missed_counts[label]  # Total de entidades verdadeiras do tipo\n",
    "    total_pred = correct_counts[label]  # Total de entidades previstas corretamente\n",
    "    \n",
    "    # Calcula precisão, recall e F1-score manualmente\n",
    "    precision_manual = correct_counts[label] / total_pred if total_pred > 0 else 0\n",
    "    recall_manual = correct_counts[label] / total_true if total_true > 0 else 0\n",
    "    f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTipo de Entidade: {label}\")\n",
    "    print(f\"Precisão Manual: {precision_manual:.2f}\")\n",
    "    print(f\"Recall Manual: {recall_manual:.2f}\")\n",
    "    print(f\"F1-Score Manual: {f1_manual:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
