{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o treinamento do modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgegobbi/programacao/Notas/.venv/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Sabão Em Pó Omo Puro Cuidado 2,2kg da marca OMO, n...\" with entities \"[(0, 34, 'PRODUCT_NAME'), (44, 47, 'BRAND'), (64, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 1/50 - Losses: {'ner': np.float32(12449.49)}\n",
      "Iteração 2/50 - Losses: {'ner': np.float32(1637.3721)}\n",
      "Iteração 3/50 - Losses: {'ner': np.float32(998.2915)}\n",
      "Iteração 4/50 - Losses: {'ner': np.float32(876.9952)}\n",
      "Iteração 5/50 - Losses: {'ner': np.float32(735.4539)}\n",
      "Iteração 6/50 - Losses: {'ner': np.float32(714.20807)}\n",
      "Iteração 7/50 - Losses: {'ner': np.float32(801.58)}\n",
      "Iteração 8/50 - Losses: {'ner': np.float32(643.26434)}\n",
      "Iteração 9/50 - Losses: {'ner': np.float32(647.2936)}\n",
      "Iteração 10/50 - Losses: {'ner': np.float32(632.73206)}\n",
      "Iteração 11/50 - Losses: {'ner': np.float32(596.15826)}\n",
      "Iteração 12/50 - Losses: {'ner': np.float32(560.543)}\n",
      "Iteração 13/50 - Losses: {'ner': np.float32(635.96423)}\n",
      "Iteração 14/50 - Losses: {'ner': np.float32(555.2048)}\n",
      "Iteração 15/50 - Losses: {'ner': np.float32(570.46606)}\n",
      "Iteração 16/50 - Losses: {'ner': np.float32(573.41736)}\n",
      "Iteração 17/50 - Losses: {'ner': np.float32(632.7118)}\n",
      "Iteração 18/50 - Losses: {'ner': np.float32(538.7089)}\n",
      "Iteração 19/50 - Losses: {'ner': np.float32(514.77094)}\n",
      "Iteração 20/50 - Losses: {'ner': np.float32(475.26703)}\n",
      "Iteração 21/50 - Losses: {'ner': np.float32(534.3661)}\n",
      "Iteração 22/50 - Losses: {'ner': np.float32(541.1259)}\n",
      "Iteração 23/50 - Losses: {'ner': np.float32(477.29623)}\n",
      "Iteração 24/50 - Losses: {'ner': np.float32(457.26675)}\n",
      "Iteração 25/50 - Losses: {'ner': np.float32(474.25586)}\n",
      "Iteração 26/50 - Losses: {'ner': np.float32(496.7433)}\n",
      "Iteração 27/50 - Losses: {'ner': np.float32(443.91556)}\n",
      "Iteração 28/50 - Losses: {'ner': np.float32(446.51306)}\n",
      "Iteração 29/50 - Losses: {'ner': np.float32(443.32758)}\n",
      "Iteração 30/50 - Losses: {'ner': np.float32(433.153)}\n",
      "Iteração 31/50 - Losses: {'ner': np.float32(467.23428)}\n",
      "Iteração 32/50 - Losses: {'ner': np.float32(403.14734)}\n",
      "Iteração 33/50 - Losses: {'ner': np.float32(452.51654)}\n",
      "Iteração 34/50 - Losses: {'ner': np.float32(446.9195)}\n",
      "Iteração 35/50 - Losses: {'ner': np.float32(432.27783)}\n",
      "Iteração 36/50 - Losses: {'ner': np.float32(450.7203)}\n",
      "Iteração 37/50 - Losses: {'ner': np.float32(424.61206)}\n",
      "Iteração 38/50 - Losses: {'ner': np.float32(449.48013)}\n",
      "Iteração 39/50 - Losses: {'ner': np.float32(471.63403)}\n",
      "Iteração 40/50 - Losses: {'ner': np.float32(433.58633)}\n",
      "Iteração 41/50 - Losses: {'ner': np.float32(434.17285)}\n",
      "Iteração 42/50 - Losses: {'ner': np.float32(411.63794)}\n",
      "Iteração 43/50 - Losses: {'ner': np.float32(426.30978)}\n",
      "Iteração 44/50 - Losses: {'ner': np.float32(403.23483)}\n",
      "Iteração 45/50 - Losses: {'ner': np.float32(374.3365)}\n",
      "Iteração 46/50 - Losses: {'ner': np.float32(411.49292)}\n",
      "Iteração 47/50 - Losses: {'ner': np.float32(510.08832)}\n",
      "Iteração 48/50 - Losses: {'ner': np.float32(437.38187)}\n",
      "Iteração 49/50 - Losses: {'ner': np.float32(462.0385)}\n",
      "Iteração 50/50 - Losses: {'ner': np.float32(363.76685)}\n",
      "Modelo de NER treinado e salvo em '../../model/spaCy_model/' com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definir os caminhos\n",
    "data_path = \"../../model/training_data/training_data.json\"\n",
    "model_output_dir = \"../../model/spaCy_model/\"\n",
    "\n",
    "# Carregar os dados de treinamento\n",
    "with open(data_path, \"r\") as file:\n",
    "    TRAINING_DATA = json.load(file)\n",
    "\n",
    "# Função para verificar e remover conflitos de sobreposição nos dados de treinamento\n",
    "def remove_overlapping_entities(data):\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations[\"entities\"]\n",
    "        entities = sorted(entities, key=lambda x: x[0])  # Ordenar as entidades pelo índice inicial\n",
    "        cleaned_entities = []\n",
    "        last_end = -1\n",
    "        for start, end, label in entities:\n",
    "            if start >= last_end:  # Verificar se não há sobreposição\n",
    "                cleaned_entities.append((start, end, label))\n",
    "                last_end = end\n",
    "        cleaned_data.append((text, {\"entities\": cleaned_entities}))\n",
    "    return cleaned_data\n",
    "\n",
    "# Remover sobreposição dos dados de treinamento\n",
    "TRAINING_DATA = remove_overlapping_entities(TRAINING_DATA)\n",
    "\n",
    "# Carregar ou criar um modelo spaCy em branco para NER\n",
    "nlp = spacy.blank(\"pt\")  # Modelo em português\n",
    "ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Adicionar os rótulos das entidades ao NER\n",
    "for _, annotations in TRAINING_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Treinamento do modelo\n",
    "optimizer = nlp.begin_training()\n",
    "n_iter = 50  # Número de iterações de treinamento\n",
    "\n",
    "print(\"Iniciando o treinamento do modelo...\")\n",
    "\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(TRAINING_DATA)  # Embaralhar dados a cada iteração\n",
    "    losses = {}\n",
    "\n",
    "    # Criar lotes de treinamento com tamanhos variáveis\n",
    "    batches = minibatch(TRAINING_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {\"entities\": annotations[\"entities\"]})\n",
    "            examples.append(example)\n",
    "        nlp.update(examples, sgd=optimizer, drop=0.5, losses=losses)\n",
    "\n",
    "    print(f\"Iteração {itn + 1}/{n_iter} - Losses: {losses}\")\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "nlp.to_disk(model_output_dir)\n",
    "print(f\"Modelo de NER treinado e salvo em '{model_output_dir}' com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
