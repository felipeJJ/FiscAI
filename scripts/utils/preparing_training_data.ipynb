{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de treinamento 'training_data.json' gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# PREPARA ARQUIVO DE TREINAMENTO COM OS DADOS DOS PRODUTOS\n",
    "\n",
    "# Carregar o arquivo JSON com os dados dos produtos processados\n",
    "with open(\"../../data/processed/products_data.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# Inicializar o conjunto de treinamento\n",
    "TRAINING_DATA = []\n",
    "\n",
    "for product in products:\n",
    "    # Extrair as informações necessárias com verificações\n",
    "    name = product.get(\"productName\", \"Produto desconhecido\")\n",
    "    brand = product.get(\"brand\", \"Marca desconhecida\")\n",
    "    categories = product.get(\"categories\", [\"Categoria desconhecida\"])\n",
    "\n",
    "    # Verificar a presença do preço com segurança\n",
    "    price = None\n",
    "    if \"sellers\" in product and product[\"sellers\"]:\n",
    "        commertial_offer = product[\"sellers\"][0].get(\"commertialOffer\")\n",
    "        if commertial_offer:\n",
    "            installment = commertial_offer.get(\"Installment\")\n",
    "            if isinstance(installment, dict):  # Verifica se Installment é um dicionário\n",
    "                price = installment.get(\"Value\")\n",
    "\n",
    "    # Verificar se productClusters existe e extrair seus valores, caso contrário usar um placeholder\n",
    "    clusters = product.get(\"productClusters\", {})\n",
    "    cluster_text = \", \".join(clusters.values()) if clusters else \"Nenhum cluster disponível\"\n",
    "    \n",
    "    # Construir a frase para treinamento, incluindo todas as categorias e ajustando a presença de preço\n",
    "    categories_text = \", \".join(categories)\n",
    "    if price is not None:\n",
    "        phrase = f\"{name} da marca {brand}, nas categorias {categories_text}, classificado como {cluster_text}, em oferta por R$ {price:.2f}.\"\n",
    "    else:\n",
    "        phrase = f\"{name} da marca {brand}, nas categorias {categories_text}, classificado como {cluster_text}.\"\n",
    "\n",
    "    # Identificar as posições das entidades na frase\n",
    "    entities = [\n",
    "        (phrase.find(name), phrase.find(name) + len(name), \"PRODUCT_NAME\"),\n",
    "        (phrase.find(brand), phrase.find(brand) + len(brand), \"BRAND\")\n",
    "    ]\n",
    "    \n",
    "    # Adicionar cada termo de categoria como uma entidade \"CATEGORY\"\n",
    "    for category in categories:\n",
    "        category_start = phrase.find(category)\n",
    "        if category_start != -1:\n",
    "            entities.append((category_start, category_start + len(category), \"CATEGORY\"))\n",
    "\n",
    "    # Adicionar cada cluster como uma entidade \"CLUSTER\"\n",
    "    cluster_start = phrase.find(cluster_text)\n",
    "    if cluster_start != -1:\n",
    "        entities.append((cluster_start, cluster_start + len(cluster_text), \"CLUSTER\"))\n",
    "    \n",
    "    # Adicionar a entidade PRICE se o preço estiver presente\n",
    "    if price is not None:\n",
    "        price_text = f\"R$ {price:.2f}\"\n",
    "        price_start = phrase.find(price_text)\n",
    "        entities.append((price_start, price_start + len(price_text), \"PRICE\"))\n",
    "\n",
    "    # Adicionar a frase processada ao conjunto de dados de treinamento\n",
    "    TRAINING_DATA.append((phrase, {\"entities\": entities}))\n",
    "\n",
    "# Salvar o conjunto de treinamento em um arquivo JSON\n",
    "with open(\"../../model/training_data/training_data.json\", \"w\") as out_file:\n",
    "    json.dump(TRAINING_DATA, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Arquivo de treinamento 'training_data.json' gerado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIDA COM O ARRAY \"CATEGORIES\" E FORMATA DE FORMA IDEAL PARA USO NO TREINAMENTO\n",
    "\n",
    "# Caminho para o arquivo de produtos\n",
    "products_data_path = \"../../data/processed/products_data.json\"\n",
    "processed_data_path = \"../../data/processed/products_data.json\"\n",
    "\n",
    "# Carregar os dados dos produtos\n",
    "with open(products_data_path, \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# Função para processar e extrair categorias únicas\n",
    "def process_categories(categories):\n",
    "    unique_terms = set()\n",
    "    for category_path in categories:\n",
    "        # Dividir a categoria nos termos individuais, ignorando barras vazias\n",
    "        terms = category_path.strip(\"/\").split(\"/\")\n",
    "        unique_terms.update(terms)\n",
    "    return list(unique_terms)\n",
    "\n",
    "# Processar cada produto e atualizar as categorias\n",
    "for product in products:\n",
    "    categories = product.get(\"categories\", [])\n",
    "    product[\"categories\"] = process_categories(categories)\n",
    "\n",
    "# Salvar o novo conjunto de dados com categorias processadas\n",
    "with open(processed_data_path, \"w\") as out_file:\n",
    "    json.dump(products, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Dados de categorias processados e salvos em 'processed_products_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos 'training_data.json' e 'validation_data.json' foram gerados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# DIVIDE OS DADOS DOS PRODUTOS PARA TREINAMENTO E TESTES\n",
    "\n",
    "# Caminho para o arquivo de dados de treinamento\n",
    "training_data_path = \"../../model/training_data/training_data.json\"\n",
    "\n",
    "# Carregar os dados do arquivo de treinamento\n",
    "with open(training_data_path, \"r\") as file:\n",
    "    all_data = json.load(file)\n",
    "\n",
    "# Embaralhar os dados para garantir a aleatoriedade\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Dividir 80% para novo conjunto de treinamento e 20% para validação\n",
    "split_index = int(0.8 * len(all_data))\n",
    "new_training_data = all_data[:split_index]\n",
    "validation_data = all_data[split_index:]\n",
    "\n",
    "# Salvar o novo conjunto de treinamento e o conjunto de validação\n",
    "with open(training_data_path, \"w\") as train_file:\n",
    "    json.dump(new_training_data, train_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "validation_data_path = \"../../data/processed/validation_data.json\"\n",
    "with open(validation_data_path, \"w\") as val_file:\n",
    "    json.dump(validation_data, val_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Arquivos 'training_data.json' e 'validation_data.json' foram gerados com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
